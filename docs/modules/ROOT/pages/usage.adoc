= Usage

Druid requires a Zookeeper to run, as well as a database.


== Setup Prerequisites

=== Zookeeper operator

Please refer to the https://github.com/stackabletech/zookeeper-operator[Zookeeper] operator and https://docs.stackable.tech/zookeeper/index.html[docs].

=== SQL Database

Druid requires a MySQL or Postgres database.

For testing purposes, you can spin up a PostgreSQL database with the bitnami PostgreSQL helm chart.  Add the bitnami repository:

    helm repo add bitnami https://charts.bitnami.com/bitnami

And set up the Postgres database:

    helm install druid bitnami/postgresql \
    --set postgresqlUsername=druid \
    --set postgresqlPassword=druid \
    --set postgresqlDatabase=druid

== Creating a Druid Cluster

With the prerequisites fulfilled, the CRD for this operator must be created:

    kubectl apply -f /etc/stackable/druid-operator/crd

Then a cluster can be deployed using the example below. Make sure you have *exactly one node* labeled with `nodeType=druid-data` as used in the deep storage selector. Having more than one data node is not supported when using local storage.


    cat <<EOF | kubectl apply -f -
apiVersion: druid.stackable.tech/v1alpha1
kind: DruidCluster
metadata:
  name: simple-druid
spec:
  version: 0.22.1
  zookeeperConfigMapName: simple-zk
  metadataStorageDatabase:
    dbType: postgresql
    connString: jdbc:postgresql://druid-postgresql/druid
    host: druid-postgresql    # this is the name of the Postgres service
    port: 5432
    user: druid
    password: druid
  deepStorage:
    storageType: local
    dataNodeSelector:
      nodeType: druid-data
    storageDirectory: /path/to/druidDeepStorage
  brokers:
    roleGroups:
      default:
        config: {}
        replicas: 1
  coordinators:
    roleGroups:
      default:
        config: {}
        replicas: 1
  historicals:
    roleGroups:
      default:
        config: {}
        replicas: 1
  middleManagers:
    roleGroups:
      default:
        config: {}
        replicas: 1
  routers:
    roleGroups:
      default:
        config: {}
        replicas: 1

The Router is hosting the web UI, a `NodePort` service is created by the operator to access the web UI. Connect to the `simple-druid-router` `NodePort` service and follow the https://druid.apache.org/docs/latest/tutorials/index.html#step-4-load-data[druid documentation] on how to load and query sample data.

== Using S3

Druid can use S3 as a backend for deep storage, as well as a data source to ingest data from.

To use S3, add a section like this to you cluster `spec`:

  s3:
    endpoint: s3-de-central.profitbricks.com
    credentialsSecret: s3-credentials

the secret `s3-credentials` should contain your access key ID and the secret access key:

  apiVersion: v1
  kind: Secret
  metadata:
    name: s3-credentials
  stringData:
    accessKeyId: YOURACCESSKEYIDHERE
    secretAccessKey: YOURSECRETACCESSKEYHERE

This allows to ingest data from accessible buckets already. To configure a bucket and prefix to be used as a deep storage, change the deep storage configuration like so:

  deepStorage:
    storageType: s3
    bucket: druid-deepstorage
    baseKey: storage           # the base key is the prefix to be used; optional

== Connecting to Druid from other Services

The operator creates a `ConfigMap` with the name of the cluster which contains connection information. Following our example above (the name of the cluster is `simple-druid`) a `ConfigMap` with the name `simple-druid` will be created containing 3 keys:

- `DRUID_ROUTER` with the format `<host>:<port>`, which points to the router processes HTTP endpoint. Here you can connect to the web UI, or use REST endpoints such as `/druid/v2/sql/` to query data. https://druid.apache.org/docs/latest/querying/sql.html#http-post[More information in the Druid Docs].
- `DRUID_AVATICA_JDBC` contains a JDBC connect string which can be used together with the https://calcite.apache.org/avatica/downloads/[Avatica JDBC Driver] to connect to Druid and query data. https://druid.apache.org/docs/latest/querying/sql.html#jdbc[More information in the Druid Docs].
- `DRUID_SQALCHEMY` contains a connection string used to connect to Druid with SQAlchemy, in - for example - Apache Superset.
