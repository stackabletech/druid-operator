= Usage

Druid requires a Zookeeper to run, as well as a database. If HDFS is used as the backend-storage (so called "deep storage") then the HDFS operator is required as well.


== Setup Prerequisites

=== Zookeeper operator

Please refer to the https://github.com/stackabletech/zookeeper-operator[Zookeeper] operator and https://docs.stackable.tech/zookeeper/index.html[docs].

=== HDFS operator (optional)

Please refer to the https://github.com/stackabletech/hdfs-operator[HDFS] operator and https://docs.stackable.tech/hdfs/index.html[docs].

=== SQL Database

Druid requires a MySQL or Postgres database.

For testing purposes, you can spin up a PostgreSQL database with the bitnami PostgreSQL helm chart.  Add the bitnami repository:

[source,bash]
----
helm repo add bitnami https://charts.bitnami.com/bitnami
----

And set up the Postgres database:

[source,bash]
----
helm install druid bitnami/postgresql \
--version=11 \
--set auth.username=druid \
--set auth.password=druid \
--set auth.database=druid
----

== Creating a Druid Cluster

With the prerequisites fulfilled, the CRD for this operator must be created:

[source,bash]
----
kubectl apply -f /etc/stackable/druid-operator/crd
----

Then a cluster can be deployed using the example below.

[source,yaml]
----
apiVersion: druid.stackable.tech/v1alpha1
kind: DruidCluster
metadata:
  name: simple-druid
spec:
  version: 0.22.1
  zookeeperConfigMapName: simple-zk
  metadataStorageDatabase:
    dbType: postgresql
    connString: jdbc:postgresql://druid-postgresql/druid
    host: druid-postgresql    # this is the name of the Postgres service
    port: 5432
    user: druid
    password: druid
  deepStorage:
    storageType: hdfs
    storageDirectory: hdfs://path/to/druidDeepStorage
  brokers:
    roleGroups:
      default:
        config: {}
        replicas: 1
  coordinators:
    roleGroups:
      default:
        config: {}
        replicas: 1
  historicals:
    roleGroups:
      default:
        config: {}
        replicas: 1
  middleManagers:
    roleGroups:
      default:
        config: {}
        replicas: 1
  routers:
    roleGroups:
      default:
        config: {}
        replicas: 1
----

The Router is hosting the web UI, a `NodePort` service is created by the operator to access the web UI. Connect to the `simple-druid-router` `NodePort` service and follow the https://druid.apache.org/docs/latest/tutorials/index.html#step-4-load-data[druid documentation] on how to load and query sample data.

== Using S3

Druid can use S3 as a backend for deep storage, as well as a data source to ingest data from.

To use S3, add a section like this to you cluster `spec`:

[source,yaml]
----
s3:
  endpoint: s3-de-central.profitbricks.com
  credentialsSecret: s3-credentials
----

the secret `s3-credentials` should contain your access key ID and the secret access key:

[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: s3-credentials
stringData:
  accessKeyId: YOURACCESSKEYIDHERE
  secretAccessKey: YOURSECRETACCESSKEYHERE
----

This allows to ingest data from accessible buckets already. To configure a bucket and prefix to be used as a deep storage, change the deep storage configuration like so:

[source,yaml]
----
deepStorage:
  storageType: s3
  bucket: druid-deepstorage
  baseKey: storage <1>
----
<1> The base key is the prefix to be used; optional

== Using Open Policy Agent (OPA) for Authorization

Druid can connect to an Open Policy Agent (OPA) instance for authorization policy decisions. You need to run an OPA instance to connect to, for which we refer to the https://docs.stackable.tech/opa/index.html[OPA Operator docs]. How you can write RegoRules for Druid is explained <<_defining_regorules, below>>.

Once you have defined your rules, you need to configure the OPA cluster name and endpoint to use for Druid authorization requests. Add a section to the `spec` for OPA:

[source,yaml]
----
opa:
  opaConfigMapName: simple-opa <1>
  opaDruidApiPath: v1/data/druid/allow <2>
----
<1> The name of your OPA cluster (`simple-opa` in this case)
<2> The resource to request. If you created a package `druid` and a rule `allow` in that package that you want to use, the resource becomes `v1/data/druid/allow`.

=== Defining RegoRules

For a general explanation of how rules are written, we refer to the https://www.openpolicyagent.org/docs/latest/#rego[OPA documentation]. Inside your rule you will have access to input from Druid. Druid provides this data to you to base your policy decisions on:

[source,json]
----
{
  "user": "someUsername", <1>
  "action": "READ", <2>
  "resource": {
    "type": "DATASOURCE", <3>
    "name": "myTable" <4>
  }
}
----
<1> The authenticated identity of the user that wants to perform the action
<2> The action type, can be either `READ` or `WRITE`.
<3> The resource type, one of `STATE`, `CONFIG` and `DATASOURCE`.
<4> In case of a datasource this is the table name, for `STATE` this will simply be `STATE`, the same for `CONFIG`.

For more details consult the https://druid.apache.org/docs/latest/operations/security-user-auth.html#authentication-and-authorization-model[Druid Authentication and Authorization Model].

== Connecting to Druid from other Services

The operator creates a `ConfigMap` with the name of the cluster which contains connection information. Following our example above (the name of the cluster is `simple-druid`) a `ConfigMap` with the name `simple-druid` will be created containing 3 keys:

- `DRUID_ROUTER` with the format `<host>:<port>`, which points to the router processes HTTP endpoint. Here you can connect to the web UI, or use REST endpoints such as `/druid/v2/sql/` to query data. https://druid.apache.org/docs/latest/querying/sql.html#http-post[More information in the Druid Docs].
- `DRUID_AVATICA_JDBC` contains a JDBC connect string which can be used together with the https://calcite.apache.org/avatica/downloads/[Avatica JDBC Driver] to connect to Druid and query data. https://druid.apache.org/docs/latest/querying/sql.html#jdbc[More information in the Druid Docs].
- `DRUID_SQALCHEMY` contains a connection string used to connect to Druid with SQAlchemy, in - for example - Apache Superset.

== Monitoring

The managed Druid instances are automatically configured to export Prometheus metrics. See
xref:home::monitoring.adoc[] for more details.

== Configuration & Environment Overrides

The cluster definition also supports overriding configuration properties and environment variables, either per role or per role group, where the more specific override (role group) has precedence over the less specific one (role).

IMPORTANT: Overriding certain properties which are set by operator (such as the HTTP port) can interfere with the operator and can lead to problems.

=== Configuration Properties

For a role or role group, at the same level of `config`, you can specify: `configOverrides` for the `runtime.properties`. For example, if you want to set the `druid.server.http.numThreads` for the router to 100 adapt the `routers` section of the cluster resource like so:

[source,yaml]
----
routers:
  roleGroups:
    default:
      config: {}
      configOverrides:
        runtime.properties:
          druid.server.http.numThreads: "100"
      replicas: 1
----

Just as for the `config`, it is possible to specify this at role level as well:

[source,yaml]
----
routers:
  configOverrides:
    runtime.properties:
      druid.server.http.numThreads: "100"
  roleGroups:
    default:
      config: {}
      replicas: 1
----

All override property values must be strings.

For a full list of configuration options we refer to the Druid https://druid.apache.org/docs/latest/configuration/index.html[Configuration Reference].

=== Environment Variables

In a similar fashion, environment variables can be (over)written. For example per role group:

[source,yaml]
----
routers:
  roleGroups:
    default:
      config: {}
      envOverrides:
        MY_ENV_VAR: "MY_VALUE"
      replicas: 1
----

or per role:

[source,yaml]
----
routers:
  envOverrides:
    MY_ENV_VAR: "MY_VALUE"
  roleGroups:
    default:
      config: {}
      replicas: 1
----


// cliOverrides don't make sense for this operator, so the feature is omitted for now