= Deep storage configuration

== [[hdfs]]HDFS

Druid can use HDFS as a backend for deep storage:

[source,yaml]
----
spec:
  clusterConfig:
    deepStorage:
      hdfs:
        configMapName: simple-hdfs # <1>
        directory: /druid # <2>
...
----
<1> Name of the HDFS cluster discovery config map. Can be supplied manually for a cluster not provided by Stackable. Needs to contain the `core-site.xml` and `hdfs-site.xml`.
<2> The directory where to store the druid data.

== [[s3]]S3

Druid can use S3 as a backend for deep storage:

[source,yaml]
----
spec:
  clusterConfig:
    deepStorage:
      s3:
        bucket:
          inline:
            bucketName: my-bucket  # <1>
            connection:
              inline:
                host: test-minio  # <2>
                port: 9000  # <3>
                credentials:  # <4>
                ...
----
<1> Bucket name.
<2> Bucket host.
<3> Optional bucket port.
<4> Credentials explained <<S3 Credentials, below>>.

It is also possible to configure the bucket connection details as a separate Kubernetes resource and only refer to that object from the `DruidCluster` like this:

[source,yaml]
----
spec:
  clusterConfig:
    deepStorage:
      s3:
        bucket:
          reference: my-bucket-resource # <1>
----
<1> Name of the bucket resource with connection details.

The resource named `my-bucket-resource` is then defined as shown below:

[source,yaml]
----
---
apiVersion: s3.stackable.tech/v1alpha1
kind: S3Bucket
metadata:
  name: my-bucket-resource
spec:
  bucketName: my-bucket-name
  connection:
    inline:
      host: test-minio
      port: 9000
      credentials:
        ... (explained below)
----

This has the advantage that bucket configuration can be shared across `DruidClusters`s (and other stackable CRDs) and reduces the cost of updating these details.

include::partial$s3-note.adoc[]

=== S3 Credentials

include::partial$s3-credentials.adoc[]