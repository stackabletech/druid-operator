= First steps
:description: Set up a Druid cluster using the Stackable Operator by installing ZooKeeper, HDFS, and Druid. Ingest and query example data via the web UI or API.

With the operators installed, deploy a Druid cluster and its dependencies.
Afterward you can <<_verify_that_it_works, verify that it works>> by ingesting example data and subsequently query it.

== Setup

Three things need to be installed to have a Druid cluster:

* A ZooKeeper instance for internal use by Druid
* An HDFS instance to be used as a backend for deep storage
* A PostgreSQL database to store the metadata of Druid
* The Druid cluster itself

Create them in this order, each one is created by applying a manifest file.
The operators you just installed then create the resources according to the manifests.

=== ZooKeeper

Create a file named `zookeeper.yaml` with the following content:

[source,yaml]

----
include::example$getting_started/zookeeper.yaml[]
----

Then create the resources by applying the manifest file:

[source,bash]
----
include::example$getting_started/getting_started.sh[tag=install-zookeeper]
----

=== HDFS

Create `hdfs.yaml` with the following contents:

[source,yaml]
----
include::example$getting_started/hdfs.yaml[]
----

And apply it:

----
include::example$getting_started/getting_started.sh[tag=install-hdfs]
----


=== PostgreSQL

Install a PostgreSQL database using `helm`.
If you already have a PostgreSQL instance, you can skip this step and use your own below.

[source,bash]
----
include::example$getting_started/getting_started.sh[tag=helm-install-postgres]
----

=== Druid

Create a file named `druid.yaml` with the following contents:

[source,yaml]
----
include::example$getting_started/druid.yaml[]
----

And apply it:

----
include::example$getting_started/getting_started.sh[tag=install-druid]
----

This creates the actual Druid Stacklet.

WARNING: This Druid instance uses Derby (`dbType: derby`) as a metadata store, which is an interal SQL database.
It is not persisted and not suitable for production use!
Consult the https://druid.apache.org/docs/latest/dependencies/metadata-storage.html#available-metadata-stores[Druid documentation] for a list of supported databases and setup instructions for production instances.

== Verify that it works

Submit an ingestion job and then query the ingested data -- either through the web interface or the API.

First, make sure that all the Pods in the StatefulSets are ready:

[source,bash]
----
kubectl get statefulset
----

The output should show all pods ready:

----
NAME                                 READY   AGE
simple-druid-broker-default          1/1     5m
simple-druid-coordinator-default     1/1     5m
simple-druid-historical-default      1/1     5m
simple-druid-middlemanager-default   1/1     5m
simple-druid-router-default          1/1     5m
simple-hdfs-datanode-default         1/1     6m
simple-hdfs-journalnode-default      1/1     6m
simple-hdfs-namenode-default         2/2     6m
simple-zk-server-default             3/3     7m
----

Ideally you use `stackablectl stacklet list` to find out the address the Druid router is reachable at and use that address.

As an alternative, you can create a port-forward for the Druid Router:

----
include::example$getting_started/getting_started.sh[tag=port-forwarding]
----

=== Ingest example data

Next, ingest some example data using the web interface.
If you prefer to use the command line instead, follow the instructions in the collapsed section below.


[#ingest-cmd-line]
.Alternative: Using the command line
[%collapsible]
====

If you prefer to not use the web interface and instead interact with the API, create a file `ingestion_spec.json` with the following contents:

[source,json]
include::example$getting_started/ingestion_spec.json[]

Submit the file with the following `curl` command:

[source,bash]

include::example$getting_started/getting_started.sh[tag=submit-job]

Continue with the <<_query_the_data,next section>>.

====

To open the web interface navigate your browser to https://localhost:9088/ to find the dashboard:

image::getting_started/dashboard.png[]

Now load the example data:

image::getting_started/load_example.png[]

Click through all pages of the load process.
You can also follow the https://druid.apache.org/docs/latest/tutorials/index.html#step-4-load-data[Druid Quickstart Guide].

Once you finished the ingestion dialog you should see the ingestion overview with the job, which eventually shows SUCCESS:

image::getting_started/load_success.png[]

=== Query the data

Query from the user interface by navigating to the "Query" interface in the menu and query the `wikipedia` table:

[#query-cmd-line]
.Alternative: Using the command line
[%collapsible]
====

To query from the commandline, create a file called `query.json` with the query:

[source,json]
include::example$getting_started/query.json[]

and execute it:

[source,bash]
include::example$getting_started/getting_started.sh[tag=query-data]

The result should be similar to:

[source,json]
include::example$getting_started/expected_query_result.json[]

====

image::getting_started/query.png[]

Great! You've set up your first Druid cluster, ingested some data and queried it in the web interface.

== What's next

Have a look at the xref:usage-guide/index.adoc[] page to find out more about the features of the operator, such as S3-backed deep storage (as opposed to the HDFS backend used in this guide) or OPA-based authorization.
