= Stackable Operator for Apache Druid
:description: The Stackable Operator for Apache Druid is a Kubernetes operator that can manage Apache Druid clusters. Learn about its features, resources, dependencies, and demos, and see the list of supported Druid versions.
:keywords: Stackable Operator, Apache Druid, Kubernetes, operator, DevOps, CRD, ZooKeeper, HDFS, S3, Kafka, Trino, OPA
:github: https://github.com/stackabletech/druid-operator/
:crd: {crd-docs-base-url}/druid-operator/{crd-docs-version}/
:crd-druidcluster: {crd-docs}/druid.stackable.tech/druidcluster/v1alpha1/
:feature-tracker: https://features.stackable.tech/unified
:druid: https://druid.apache.org/
:druid-processes: https://druid.apache.org/docs/latest/design/processes.html
:pegelonline: https://www.pegelonline.wsv.de/webservice/ueberblick
:earthquake: https://earthquake.usgs.gov/

[.link-bar]
* {github}[GitHub {external-link-icon}^]
* {feature-tracker}[Feature Tracker {external-link-icon}^]
* {crd}[CRD documentation {external-link-icon}^]

The Stackable operator for Apache Druid is an operator that can deploy and manage {druid}[Apache Druid] clusters on Kubernetes.
Apache Druid is an open-source, distributed data store designed to quickly process large amounts of data in real-time.
It enables users to ingest, store, and query massive amounts of data in real-time, a great tool for handling high-volume data processing and analysis.
This operator provides several resources and features to manage Druid clusters efficiently.

== Getting started

To get started with the Stackable operator for Apache Druid, follow the xref:druid:getting_started/index.adoc[Getting Started guide].
It guides you through the installation of the operator and its dependencies (ZooKeeper, HDFS, an SQL database) and the steps to query your first sample data.

== Resources

The operator is installed along with the _DruidCluster_ CustomResourceDefinition, which supports five
xref:concepts:roles-and-role-groups.adoc[roles]: **Router**, **Coordinator**, **Broker**, **MiddleManager** and
**Historical**. These roles correspond to {druid-processes}[Druid processes].

The operator watches DruidCluster objects and creates multiple Kubernetes resources for each DruidCluster based on its configuration.

image::druid_overview.drawio.svg[A diagram depicting the Kubernetes resources created by the operator]

For every RoleGroup a **StatefulSet** is created.
Each StatefulSet can contain multiple replicas (Pods).
Each Pod has at least two containers: the main Druid container and a preparation container which just runs once at startup.
If xref:usage-guide/logging.adoc[] is enabled, there is a sidecar container for logging too.
For every Role and RoleGroup the operator creates a **Service**.

A **ConfigMap** is created for each RoleGroup containing 3 files:
`jvm.config` and `runtime.properties` files generated from the DruidCluster configuration (See xref:usage-guide/index.adoc[] for more information),
plus a `log4j2.properties` file used for xref:usage-guide/logging.adoc[].
For the whole DruidCluster a **xref:reference/discovery.adoc[discovery ConfigMap]** is created which contains information on how to connect to the Druid cluster.

== Dependencies and other operators to connect to

The Druid operator has the following dependencies:

* A xref:usage-guide/deep-storage.adoc[deep storage] backend is required to persist data. Use either
  xref:usage-guide/deep-storage.adoc#hdfs[HDFS] with the xref:hdfs:index.adoc[] or
  xref:usage-guide/deep-storage.adoc#s3[S3].
* An SQL database to store metadata.
* Apache ZooKeeper via the xref:zookeeper:index.adoc[]. Apache ZooKeeper is used by Druid for internal communication
  between processes.
* The xref:commons-operator:index.adoc[] provides common CRDs such as xref:concepts:s3.adoc[] CRDs.
* The xref:secret-operator:index.adoc[] is required for things like S3 access credentials or LDAP integration.
* The xref:listener-operator:index.adoc[] exposes the pods to the outside network.

Have a look at the xref:getting_started/index.adoc[getting started guide] for an example of a minimal working setup.

The getting started guide sets up a fully working Druid cluster, but the S3 deep storage backend as well as the metadata
SQL database are xref:required-external-components.adoc[required external components] and need to be set up by you as
prerequisites for a production setup.

Druid works well with other Stackable supported products, such as xref:kafka:index.adoc[Apache Kafka] for data ingestion
xref:trino:index.adoc[Trino] for data processing or xref:superset:index.adoc[Apache Superset] for data visualization.
xref:opa:index.adoc[OPA] can be connected to create authorization policies. Have a look at the
xref:usage-guide/index.adoc[] for more configuration options and have a look at the <<demos, demos>> for complete data
pipelines you can install with a single command.

== [[demos]]Demos

xref:management:stackablectl:index.adoc[] supports installing xref:demos:index.adoc[] with a single command.
The demos are complete data piplines which showcase multiple components of the Stackable platform working together and which you can try out interactively.
Both demos below include Druid as part of the data pipeline:

=== Waterlevel demo

The xref:demos:nifi-kafka-druid-water-level-data.adoc[] demo uses data from {pegelonline}[PEGELONLINE] to visualize water levels in rivers and coastal regions of Germany from historic and real time data.

=== Earthquake demo

The xref:demos:nifi-kafka-druid-earthquake-data.adoc[] demo ingests {earthquake}[earthquake data] into a similar pipeline as is used in the waterlevel demo.

== Supported versions

The Stackable operator for Apache Druid currently supports the Druid versions listed below.
To use a specific Druid version in your DruidCluster, you have to specify an image - this is explained in the xref:concepts:product-image-selection.adoc[] documentation.
The operator also supports running images from a custom registry or running entirely customized images; both of these cases are explained under xref:concepts:product-image-selection.adoc[] as well.

include::partial$supported-versions.adoc[]

== Useful links

* The {github}[druid-operator {external-link-icon}^] GitHub repository
* The operator feature overview in the {feature-tracker}[feature tracker {external-link-icon}^]
* The {crd-druidcluster}[DruidCluster {external-link-icon}^] CRD documentation
