= Stackable Operator for Apache Druid
:description: The Stackable Operator for Apache Druid is a Kubernetes operator that can manage Apache Druid clusters. Learn about its features, resources, dependencies, and demos, and see the list of supported Druid versions.
:keywords: Stackable Operator, Apache Druid, Kubernetes, operator, DevOps, engineer, CRD, StatefulSet, ConfigMap, Service, ZooKeeper, HDFS, S3, Kafka, Trino, OPA, demo, version

The Stackable Operator for Apache Druid is a Kubernetes operator that can manage https://druid.apache.org/[Apache Druid] clusters. With this operator you can deploy and manage Druid clusters on Kubernetes. This operator provides several resources and features to manage Druid clusters efficiently.

== Getting Started

To get started with the Stackable Operator for Apache Druid, follow the xref:druid:getting_started/index.adoc[Getting Started guide]. The Operator is installed along with the _DruidCluster_ CustomResourceDefinition, which supports five xref:home:concepts:roles-and-role-groups.adoc[roles]: **Router**, **Coordinator**, **Broker**, **MiddleManager** and **Historical**. These roles correspond to https://druid.apache.org/docs/latest/design/processes.html[Druid processes].

== Resources

The Operator watches DruidCluster objects and creates multiple Kubernetes resources for each DruidCluster based its configuration.

image::druid_overview.drawio.svg[A diagram depicting the Kubernetes resources the operator creates]

For every RoleGroup a **StatefulSet** is created. Each StatefulSet can contain multiple replicas (Pods). each Pod has at least two containers: the main Druid container and a preparation container (TODO what for exactly?). If xref:usage-guide/logging.adoc[] is enabled, there is a sidecar container for logging too.
For every Role and RoleGroup the Operator creates a **Service**.

A **ConfigMap** is created for each RoleGroup containing 3 files. A generated `jvm.config` and `runtime.properties` file based on the DruidCluster configuration (See xref:usage-guide/index.adoc[] for more information). A `log4j2.properties` file used for xref:usage-guide/logging.adoc[].
For the whole DruidCluster a **xref:discovery.adoc[discovery ConfigMap]** is created which contains information on how to connect to the Druid cluster.

== Dependencies and other Operators to connect to

The Druid Operator has the following dependencies:

* The xref:commons-operator:index.adoc[] provides common CRDs such as xref:concepts:s3.adoc[] CRDs.
* The xref:secret-operator:index.adoc[] is required for things like S3 access credentials or LDAP integration.
* Apache ZooKeeper via the xref:zookeeper:index.adoc[]. Apache ZooKeeper is used by Druid for internal communication between processes.
* A storage backend. Either HDFS with the xref:hdfs:index.adoc[] or S3. (TODO: link to S3 deep storage config usage).
* An SQL database to store metadata.

You can ingest data from Kafka with the xref:kafka:index.adoc[].

Trino with the xref:trino:index.adoc[] and Superset (xref:superset:index.adoc[]) can read from Druid (have a look at the <<demos, demos>>).

OPA with the xref:opa:index.adoc[] for Authorization.

== [[demos]]Demos

xref:stackablectl::index.adoc[] supports installing xref:stackablectl::demos/index.adoc[] which showcase multiple components of the Stackable platform working together. These two demos include Druid as part of the data pipeline:

=== Waterlevel Demo

The xref:stackablectl::demos/nifi-kafka-druid-water-level-data.adoc[] demo uses data from https://www.pegelonline.wsv.de/webservice/ueberblick[PEGELONLINE] to visualize water levels in rivers and coastal regions of Germany from historic and real time data.

=== Earthquake Demo

The xref:stackablectl::demos/nifi-kafka-druid-earthquake-data.adoc[] demo ingests https://earthquake.usgs.gov/[earthquake data] into a similar pipeline as is used in the waterlevel demo.


== Supported Versions

The Stackable Operator for Apache Druid currently supports the following versions of Druid:

include::partial$supported-versions.adoc[]
