= First cluster

== Setup

After going through the xref:installation.adoc[] section and have installed all the Operators, you can deploy product instances. You will also install an HDFS instance which is used as a deep storage backend by Druid, as well as a ZooKeeper instance which is needed by both Druid and HDFS.

=== ZooKeeper

Create a file named `zookeeper.yaml` with the following content:

[source,yaml]

----
include::example$code/zookeeper.yaml[]
----

Then create the resources by applying the manifest file

[source,bash]
----
include::example$code/getting-started.sh[tag=install-zookeeper]
----

The command should output this:
----
zookeepercluster.zookeeper.stackable.tech/simple-zk created
zookeeperznode.zookeeper.stackable.tech/simple-druid-znode created
zookeeperznode.zookeeper.stackable.tech/simple-hdfs-znode created
----

=== HDFS

Druid needs a data storage location, we will use HDFS.

Create the file:

[source,yaml]
----
include::example$code/hdfs.yaml[]
----


----
include::example$code/getting-started.sh[tag=install-hdfs]
----

=== Druid

Create a file named `druid.yaml` with the following content:

[source,yaml]
----
include::example$code/druid.yaml[]
----

----
include::example$code/getting-started.sh[tag=install-druid]
----

== Verify that it works

Next you will submit an ingestion job and then query the ingested data  - either through the web interface or the API.

First, make sure that all the Pods in the StatefulSets are ready:

[source,bash]
----
kubectl get statefulset
----

The output should show all pods ready:

----
NAME                                 READY   AGE
simple-druid-broker-default          1/1     5m
simple-druid-coordinator-default     1/1     5m
simple-druid-historical-default      1/1     5m
simple-druid-middlemanager-default   1/1     5m
simple-druid-router-default          1/1     5m
simple-hdfs-datanode-default         1/1     6m
simple-hdfs-journalnode-default      1/1     6m
simple-hdfs-namenode-default         2/2     6m
simple-zk-server-default             3/3     7m
----

Then, port-forward the Druid Router:

----
include::example$code/getting-started.sh[tag=port-forwarding]
----

=== Ingest example data

Next, we will ingest some example data using the web interface.

Navigate your browser to https://localhost:8888/ to find the dashboard:

image::dashboard.png[]

Now load the example data:

image::load_example.png[]

Click through all pages of the load process. You can also follow the https://druid.apache.org/docs/latest/tutorials/index.html#step-4-load-data[Druid Quickstart Guide].

Once you finished the ingestion dialog you should see the ingestion overview with the job, which will eventually show SUCCESS:

image::load_success.png[]

.Alternative: Using the command line
[%collapsible]
====

If you prefer to not use the user interface and instead interact with the API, create a file `ingestion_spec.json` with the following contents:

[source,json]
include::example$code/ingestion_spec.json[]

Submit the file with the following `curl` command:

[source,bash]
include::example$code/getting-started.sh[tag=submit-job]

====


=== Query the data

You can now navigate to "Query" in the menu and query the `wikipedia` table:

image::query.png[]

Great! You've set up your first Druid cluster, ingested some data and queried it in the web interface!

== What's next

Have a look at the xref::usage.adoc[] page to find out more about features.